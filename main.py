# main.py
import io
import re
import unicodedata
from pathlib import Path

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import streamlit as st


# =========================
# Page / Global Style
# =========================
st.set_page_config(page_title="ğŸŒ± ê·¹ì§€ì‹ë¬¼ ìµœì  EC/ì˜¨ë„ ì—°êµ¬", layout="wide")

st.markdown(
    """
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap');
html, body, [class*="css"] {
    font-family: 'Noto Sans KR', 'Malgun Gothic', sans-serif;
}
</style>
""",
    unsafe_allow_html=True,
)


def apply_plotly_korean_font(fig: go.Figure) -> go.Figure:
    fig.update_layout(
        font=dict(family="Malgun Gothic, Apple SD Gothic Neo, Noto Sans KR, sans-serif"),
        legend=dict(title=None),
        margin=dict(l=30, r=30, t=70, b=40),
    )
    return fig


# =========================
# Experiment Constants (display & fallback)
# =========================
SCHOOL_ORDER = ["ì†¡ë„ê³ ", "í•˜ëŠ˜ê³ ", "ì•„ë¼ê³ ", "ë™ì‚°ê³ "]

# ë³´ê³ ì„œ(í…ìŠ¤íŠ¸)ì—ì„œ ì œì‹œëœ ëŒ€í‘œê°’(í•™êµë³„ í‰ê·  ì¶”ì •ì¹˜)
# - ì˜¨ë„ í‰ê· : ì†¡ë„ 23.54, ë™ì‚° 22.37, í•˜ëŠ˜ 18.18, ì•„ë¼ê³  19.26
# - EC í‰ê· : ì†¡ë„ 0.72, ë™ì‚° 1.11, í•˜ëŠ˜ 4.00, ì•„ë¼ê³  7.82
# - ìƒì¤‘ëŸ‰ í‰ê· : í•˜ëŠ˜ 3.94, ì†¡ë„ 3.73, ë™ì‚° 3.53, ì•„ë¼ê³  1.89
REPORT_FALLBACK = pd.DataFrame(
    {
        "í•™êµ": ["ì†¡ë„ê³ ", "ë™ì‚°ê³ ", "í•˜ëŠ˜ê³ ", "ì•„ë¼ê³ "],
        "í‰ê· ì˜¨ë„": [23.54, 22.37, 18.18, 19.26],
        "í‰ê· EC": [0.72, 1.11, 4.00, 7.82],
        "í‰ê· ìƒì¤‘ëŸ‰": [3.73, 3.53, 3.94, 1.89],
    }
)

# ì‚¬ì´ë“œë°”ì—ì„œ "í•™êµ ì„ íƒ" ìš©(ì „ì²´ í¬í•¨)
SCHOOL_SELECT = ["ì „ì²´"] + SCHOOL_ORDER


# =========================
# File / Unicode Robust
# =========================
def get_data_dir() -> Path:
    """
    Streamlit Cloud/ë¡œì»¬ì—ì„œ data í´ë”ë¥¼ í™•ì‹¤íˆ ì°¾ëŠ”ë‹¤.
    - main.py ìœ„ì¹˜ ê¸°ì¤€ ./data ìš°ì„ 
    - ê·¸ ë‹¤ìŒ í˜„ì¬ ì‘ì—…í´ë” ./data
    """
    here = Path(__file__).resolve().parent
    cand1 = here / "data"
    if cand1.exists():
        return cand1

    cand2 = Path.cwd() / "data"
    if cand2.exists():
        return cand2

    return cand1


def _norm_all(s: str) -> set[str]:
    return {
        unicodedata.normalize("NFC", s),
        unicodedata.normalize("NFD", s),
    }


def canonical_filename(name: str) -> str:
    n = unicodedata.normalize("NFC", str(name)).strip()
    low = n.lower()
    if low.endswith(".csv.csv"):
        n = n[:-4]
    if low.endswith(".xlsx.xlsx"):
        n = n[:-5]
    return n


def filename_match(candidate: str, desired: str) -> bool:
    c_nfc = canonical_filename(candidate)
    d_nfc = canonical_filename(desired)
    c_nfd = unicodedata.normalize("NFD", c_nfc)
    d_nfd = unicodedata.normalize("NFD", d_nfc)

    if c_nfc == d_nfc or c_nfd == d_nfd:
        return True
    if c_nfc.endswith(d_nfc) or c_nfd.endswith(d_nfd):
        return True
    return False


def find_file_by_name(directory: Path, desired_name: str) -> Path | None:
    if not directory.exists():
        return None

    desired_norms = _norm_all(canonical_filename(desired_name))
    for p in directory.iterdir():
        if not p.is_file():
            continue

        cand_name = canonical_filename(p.name)
        cand_norms = _norm_all(cand_name)

        if desired_norms.intersection(cand_norms):
            return p
        if filename_match(p.name, desired_name):
            return p
    return None


# =========================
# CSV Safety & Column Standardization
# =========================
def read_csv_safely(path: Path) -> pd.DataFrame:
    encodings = ["utf-8-sig", "utf-8", "cp949", "euc-kr"]
    last_err = None
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception as e:
            last_err = e
    raise last_err


def normalize_colname(c: str) -> str:
    c = unicodedata.normalize("NFC", str(c)).strip().lower()
    c = c.replace("\ufeff", "")
    c = re.sub(r"\s+", "", c)
    c = c.replace("-", "").replace(".", "")
    return c


def standardize_env_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    env: time, temperature, humidity, ph, ec ë¡œ í‘œì¤€í™”
    """
    df2 = df.copy()
    colmap = {c: normalize_colname(c) for c in df2.columns}
    inv = {}
    for orig, n in colmap.items():
        inv.setdefault(n, []).append(orig)

    target = {
        "time": {"time", "datetime", "timestamp", "ì¸¡ì •ì‹œê°„", "ì‹œê°„", "date", "ë‚ ì§œ"},
        "temperature": {"temperature", "temp", "t", "ì˜¨ë„"},
        "humidity": {"humidity", "hum", "h", "ìŠµë„"},
        "ph": {"ph", "ì‚°ë„"},
        "ec": {"ec", "ì „ê¸°ì „ë„ë„"},
    }

    rename = {}
    for std, cands in target.items():
        found = None
        for cand in cands:
            if cand in inv:
                found = inv[cand][0]
                break
        if found is not None:
            rename[found] = std

    return df2.rename(columns=rename)


def ensure_datetime(df: pd.DataFrame, time_col: str) -> pd.DataFrame:
    out = df.copy()
    out[time_col] = pd.to_datetime(out[time_col], errors="coerce")
    out = out.dropna(subset=[time_col])
    return out.sort_values(time_col)


# =========================
# Growth Helpers
# =========================
def pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:
    cols = list(df.columns)
    norm_map = {unicodedata.normalize("NFC", str(c)): c for c in cols}
    for cand in candidates:
        cand_nfc = unicodedata.normalize("NFC", cand)
        if cand_nfc in norm_map:
            return norm_map[cand_nfc]
    return None


# =========================
# Data Loading (cached)
# =========================
@st.cache_data(show_spinner=False)
def load_environment_data(data_dir: Path) -> dict[str, pd.DataFrame]:
    env = {}
    desired_files = {
        "ì†¡ë„ê³ ": "ì†¡ë„ê³ _í™˜ê²½ë°ì´í„°.csv",
        "í•˜ëŠ˜ê³ ": "í•˜ëŠ˜ê³ _í™˜ê²½ë°ì´í„°.csv",
        "ì•„ë¼ê³ ": "ì•„ë¼ê³ _í™˜ê²½ë°ì´í„°.csv",
        "ë™ì‚°ê³ ": "ë™ì‚°ê³ _í™˜ê²½ë°ì´í„°.csv",
    }

    for school, desired_name in desired_files.items():
        p = find_file_by_name(data_dir, desired_name)
        if p is None:
            continue

        df = read_csv_safely(p)
        df = standardize_env_columns(df)
        df["í•™êµ"] = school
        env[school] = df

    return env


@st.cache_data(show_spinner=False)
def load_growth_data(data_dir: Path) -> tuple[pd.DataFrame, list[str], Path | None]:
    desired_name = "4ê°œêµ_ìƒìœ¡ê²°ê³¼ë°ì´í„°.xlsx"
    xlsx_path = find_file_by_name(data_dir, desired_name)
    if xlsx_path is None:
        xlsx_path = find_file_by_name(data_dir, "4ê°œêµ_ìƒìœ¡ê²°ê³¼ë°ì´í„°.xlsx.xlsx")

    if xlsx_path is None:
        return pd.DataFrame(), [], None

    xls = pd.ExcelFile(xlsx_path)
    sheet_names = list(xls.sheet_names)  # ì‹œíŠ¸ëª… í•˜ë“œì½”ë”© ê¸ˆì§€

    frames = []
    for sh in sheet_names:
        df = pd.read_excel(xls, sheet_name=sh, engine="openpyxl")
        df["í•™êµ"] = sh
        frames.append(df)

    all_df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    return all_df, sheet_names, xlsx_path


# =========================
# Metrics Builder (real data -> fallback)
# =========================
def build_school_metrics(
    env_by_school: dict[str, pd.DataFrame],
    growth_df: pd.DataFrame,
) -> pd.DataFrame:
    """
    í•™êµë³„ í‰ê· ì˜¨ë„/í‰ê· EC/í‰ê· ìƒì¤‘ëŸ‰ì„ ë§Œë“ ë‹¤.
    - ê°€ëŠ¥í•˜ë©´ ì‹¤ì œ CSV/XLSXì—ì„œ ê³„ì‚°
    - ë¶€ì¡±í•˜ë©´ ë³´ê³ ì„œ ëŒ€í‘œê°’ìœ¼ë¡œ ì±„ì›€
    """
    # 1) í™˜ê²½ í‰ê·  (ì‹¤ë°ì´í„°)
    env_rows = []
    for s in SCHOOL_ORDER:
        df = env_by_school.get(s)
        if df is None or df.empty:
            continue
        if "temperature" not in df.columns or "ec" not in df.columns:
            continue
        t_mean = pd.to_numeric(df["temperature"], errors="coerce").mean()
        ec_mean = pd.to_numeric(df["ec"], errors="coerce").mean()
        env_rows.append({"í•™êµ": s, "í‰ê· ì˜¨ë„": t_mean, "í‰ê· EC": ec_mean})
    env_mean = pd.DataFrame(env_rows)

    # 2) ìƒì¤‘ëŸ‰ í‰ê·  (ì‹¤ë°ì´í„°)
    g_mean = pd.DataFrame()
    if growth_df is not None and not growth_df.empty and "í•™êµ" in growth_df.columns:
        col_weight = pick_col(growth_df, ["ìƒì¤‘ëŸ‰(g)", "ìƒì¤‘ëŸ‰"])
        if col_weight is not None:
            tmp = growth_df.copy()
            tmp[col_weight] = pd.to_numeric(tmp[col_weight], errors="coerce")
            g_mean = (
                tmp.groupby("í•™êµ", dropna=False)[col_weight]
                .mean()
                .reset_index()
                .rename(columns={col_weight: "í‰ê· ìƒì¤‘ëŸ‰"})
            )

    # 3) ë³‘í•© í›„ ëˆ„ë½ì€ fallbackìœ¼ë¡œ ì±„ìš°ê¸°
    m = pd.DataFrame({"í•™êµ": SCHOOL_ORDER})
    if not env_mean.empty:
        m = m.merge(env_mean, on="í•™êµ", how="left")
    else:
        m["í‰ê· ì˜¨ë„"] = pd.NA
        m["í‰ê· EC"] = pd.NA

    if not g_mean.empty:
        m = m.merge(g_mean, on="í•™êµ", how="left")
    else:
        m["í‰ê· ìƒì¤‘ëŸ‰"] = pd.NA

    # fallback join
    fb = REPORT_FALLBACK.copy()
    m = m.merge(fb, on="í•™êµ", how="left", suffixes=("", "_fb"))

    for col in ["í‰ê· ì˜¨ë„", "í‰ê· EC", "í‰ê· ìƒì¤‘ëŸ‰"]:
        m[col] = m[col].astype("float64")
        fb_col = f"{col}_fb"
        m[col] = m[col].fillna(m[fb_col])

    m = m[["í•™êµ", "í‰ê· ì˜¨ë„", "í‰ê· EC", "í‰ê· ìƒì¤‘ëŸ‰"]]
    m["í•™êµ"] = pd.Categorical(m["í•™êµ"], categories=SCHOOL_ORDER, ordered=True)
    m = m.sort_values("í•™êµ").reset_index(drop=True)
    return m


# =========================
# Sidebar
# =========================
st.title("ğŸŒ± ê·¹ì§€ì‹ë¬¼ ìµœì  EC ë†ë„ ì—°êµ¬")

data_dir = get_data_dir()

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    school_option = st.selectbox("í•™êµ ì„ íƒ", SCHOOL_SELECT, index=0)

    with st.expander("ğŸ§ª ë””ë²„ê·¸: data í´ë”/íŒŒì¼ í™•ì¸"):
        st.write("data_dir =", str(data_dir))
        if data_dir.exists():
            st.write([p.name for p in data_dir.iterdir() if p.is_file()])
        else:
            st.error("data í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


# =========================
# Load Data
# =========================
with st.spinner("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
    env_by_school = load_environment_data(data_dir)
    growth_df, sheet_names, growth_path = load_growth_data(data_dir)

if growth_df is None or growth_df.empty:
    st.error("ìƒìœ¡ ê²°ê³¼ XLSXë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. data/ì— '4ê°œêµ_ìƒìœ¡ê²°ê³¼ë°ì´í„°.xlsx'ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

missing_env = [s for s in SCHOOL_ORDER if s not in env_by_school]
if missing_env:
    st.warning(f"í™˜ê²½ ë°ì´í„°ê°€ ì—†ëŠ” í•™êµ: {', '.join(missing_env)} (data/ íŒŒì¼ëª… ë˜ëŠ” ì¸ì½”ë”©/ì»¬ëŸ¼ í™•ì¸)")

metrics = build_school_metrics(env_by_school, growth_df)

# school filter
metrics_show = metrics.copy()
if school_option != "ì „ì²´":
    metrics_show = metrics_show[metrics_show["í•™êµ"] == school_option].copy()

# =========================
# Tabs (ìš”êµ¬ì‚¬í•­ 3ê°œ)
# =========================
tab1, tab2, tab3 = st.tabs(
    ["ğŸ“– ì‹¤í—˜ ê°œìš”", "ğŸ“Š í•™êµë³„ ì˜¨ë„Â·EC ë§‰ëŒ€ê·¸ë˜í”„", "ğŸ” ìƒì¤‘ëŸ‰Â·ECÂ·ì˜¨ë„ ìƒê´€ê´€ê³„(ìœµí•©)"]
)

# -------------------------
# Tab 1: ì‹¤í—˜ê°œìš” (ë³´ê³ ì„œ ê¸°ë°˜)
# -------------------------
with tab1:
    st.subheader("1) ì‹¤í—˜ ê°œìš”(ë³´ê³ ì„œ ìš”ì•½ ê¸°ë°˜)")
    st.write(
        """
**ëŒ€ìƒ ì‹ë¬¼:** ê·¹ì§€ ëª¨ë¸ì‹ë¬¼ â€˜ë‚˜ë„ìˆ˜ì˜â€™  
**ëª©í‘œ:** 4ê°œ ê³ ë“±í•™êµì—ì„œ ìˆ˜ì§‘í•œ í™˜ê²½(ì˜¨ë„Â·EC ë“±)ê³¼ ìƒìœ¡(ìƒì¤‘ëŸ‰ ë“±) ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬,
ê·¹ì§€ì‹ë¬¼ì´ ê°€ì¥ ì˜ ìë¼ëŠ” **ìµœì  í™˜ê²½ ë²”ìœ„**ë¥¼ ê·œëª…í•œë‹¤.

**í•µì‹¬ ê²°ë¡ (ë³´ê³ ì„œ):**
- ê·¹ì§€ì‹ë¬¼ì€ ëŒ€ì²´ë¡œ **18~22â„ƒ** ë²”ìœ„ì—ì„œ ë¬´ë‚œí•˜ê²Œ ìëìœ¼ë©°,
- íŠ¹íˆ **EC 3~4 mS/cm** êµ¬ê°„ì—ì„œ ìƒì¤‘ëŸ‰ì´ ìµœëŒ€ì´ê³ ,
- ì´ë²ˆ ë°ì´í„°ì—ì„œëŠ” **ì˜¨ë„ë³´ë‹¤ ECê°€ ìƒìœ¡ì— ë” í° ì˜í–¥**ì„ ë³´ì˜€ë‹¤.
"""
    )

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("ì°¸ì—¬ í•™êµ", f"{len(SCHOOL_ORDER)}ê°œ")
    c2.metric("ìµœì  ì˜¨ë„(ë³´ê³ ì„œ)", "18~22â„ƒ")
    c3.metric("ìµœì  EC(ë³´ê³ ì„œ)", "3~4 mS/cm")
    # ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œë„ ìµœëŒ“ê°’(í‰ê· ìƒì¤‘ëŸ‰ ìµœëŒ€) í™•ì¸
    best_row = metrics.sort_values("í‰ê· ìƒì¤‘ëŸ‰", ascending=False).head(1)
    if not best_row.empty:
        best_school = str(best_row.iloc[0]["í•™êµ"])
        best_w = float(best_row.iloc[0]["í‰ê· ìƒì¤‘ëŸ‰"])
        c4.metric("ë°ì´í„° ê¸°ì¤€ ìƒì¤‘ëŸ‰ 1ìœ„", f"{best_w:.2f} g", best_school)

    st.divider()
    st.subheader("í•™êµë³„ ëŒ€í‘œê°’(ëŒ€ì‹œë³´ë“œ ê³„ì‚°ìš©)")
    st.dataframe(metrics, use_container_width=True)

# -------------------------
# Tab 2: í•™êµë³„ ì˜¨ë„ & EC ë§‰ëŒ€ê·¸ë˜í”„ (ìš”êµ¬ì‚¬í•­ 2)
# -------------------------
with tab2:
    st.subheader("2) í•™êµë³„ í‰ê·  ì˜¨ë„ì™€ í‰ê·  EC(ë§‰ëŒ€ê·¸ë˜í”„)")
    if metrics_show.empty:
        st.error("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("í‰ê·  ì˜¨ë„(â„ƒ)", "í‰ê·  EC(mS/cm)"),
        )
        fig.add_trace(go.Bar(x=metrics_show["í•™êµ"], y=metrics_show["í‰ê· ì˜¨ë„"], name="í‰ê·  ì˜¨ë„"), row=1, col=1)
        fig.add_trace(go.Bar(x=metrics_show["í•™êµ"], y=metrics_show["í‰ê· EC"], name="í‰ê·  EC"), row=1, col=2)

        fig.update_layout(height=520, title="í•™êµë³„ í™˜ê²½ ì¡°ê±´ ë¹„êµ")
        fig = apply_plotly_korean_font(fig)
        st.plotly_chart(fig, use_container_width=True)

# -------------------------
# Tab 3: ìƒì¤‘ëŸ‰Â·ECÂ·ì˜¨ë„ ìƒê´€ê´€ê³„(ì‚°ì ë„ + êº¾ì€ì„  ìœµí•©) (ìš”êµ¬ì‚¬í•­ 3)
# -------------------------
with tab3:
    st.subheader("3) ìƒì¤‘ëŸ‰Â·ECÂ·ì˜¨ë„ ìƒê´€ê´€ê³„(ì‚°ì ë„ + êº¾ì€ì„  ìœµí•© í‘œí˜„)")

    if metrics_show.empty:
        st.error("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # EC ê¸°ì¤€ ì •ë ¬(êº¾ì€ì„  ì—°ê²°ì„ ìœ„í•´)
    mline = metrics_show.copy()
    mline = mline.sort_values("í‰ê· EC").reset_index(drop=True)

    left, right = st.columns([1.15, 1])

    with left:
        st.markdown("### âœ… ì‚°ì ë„(EC â†” ìƒì¤‘ëŸ‰) + ì˜¨ë„ ë°˜ì˜(ë§ˆì»¤ í¬ê¸°)")
        fig_sc = px.scatter(
            mline,
            x="í‰ê· EC",
            y="í‰ê· ìƒì¤‘ëŸ‰",
            color="í•™êµ",
            size="í‰ê· ì˜¨ë„",  # ì˜¨ë„ê¹Œì§€ ë°˜ì˜
            hover_data={"í‰ê· ì˜¨ë„": ":.2f", "í‰ê· EC": ":.2f", "í‰ê· ìƒì¤‘ëŸ‰": ":.2f"},
            labels={"í‰ê· EC": "í‰ê·  EC(mS/cm)", "í‰ê· ìƒì¤‘ëŸ‰": "í‰ê·  ìƒì¤‘ëŸ‰(g)", "í‰ê· ì˜¨ë„": "í‰ê·  ì˜¨ë„(â„ƒ)"},
            title="EC-ìƒì¤‘ëŸ‰ ê´€ê³„(ì˜¨ë„ê¹Œì§€ ë™ì‹œì— ë°˜ì˜)",
        )
        # ECê°€ 3~4 ê·¼ì²˜ë¥¼ â€œê¶Œì¥ êµ¬ê°„â€ìœ¼ë¡œ ì‹œê°ì  ê°€ì´ë“œ(ë³´ê³ ì„œ ê²°ë¡  ë°˜ì˜)
        fig_sc.add_vrect(x0=3, x1=4, opacity=0.15, annotation_text="ê¶Œì¥ EC(3~4)", annotation_position="top left")
        fig_sc = apply_plotly_korean_font(fig_sc)
        st.plotly_chart(fig_sc, use_container_width=True)

    with right:
        st.markdown("### âœ… ìœµí•© êº¾ì€ì„ (ì´ì¤‘ì¶•): x=EC / y1=ìƒì¤‘ëŸ‰ / y2=ì˜¨ë„")
        fig_mix = make_subplots(specs=[[{"secondary_y": True}]])

        # ìƒì¤‘ëŸ‰(ì¢Œì¶•)
        fig_mix.add_trace(
            go.Scatter(
                x=mline["í‰ê· EC"],
                y=mline["í‰ê· ìƒì¤‘ëŸ‰"],
                mode="lines+markers",
                name="í‰ê·  ìƒì¤‘ëŸ‰(g)",
            ),
            secondary_y=False,
        )

        # ì˜¨ë„(ìš°ì¶•)
        fig_mix.add_trace(
            go.Scatter(
                x=mline["í‰ê· EC"],
                y=mline["í‰ê· ì˜¨ë„"],
                mode="lines+markers",
                name="í‰ê·  ì˜¨ë„(â„ƒ)",
            ),
            secondary_y=True,
        )

        # ê¶Œì¥ EC êµ¬ê°„ ê°•ì¡°(3~4)
        fig_mix.add_vrect(x0=3, x1=4, opacity=0.15, annotation_text="ê¶Œì¥ EC(3~4)", annotation_position="top left")

        fig_mix.update_xaxes(title_text="í‰ê·  EC(mS/cm)")
        fig_mix.update_yaxes(title_text="í‰ê·  ìƒì¤‘ëŸ‰(g)", secondary_y=False)
        fig_mix.update_yaxes(title_text="í‰ê·  ì˜¨ë„(â„ƒ)", secondary_y=True)
        fig_mix.update_layout(height=520, title="ECë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒì¤‘ëŸ‰Â·ì˜¨ë„ë¥¼ ë™ì‹œì— í•´ì„(ìœµí•©)")
        fig_mix = apply_plotly_korean_font(fig_mix)

        st.plotly_chart(fig_mix, use_container_width=True)

    st.divider()
    st.subheader("í•´ì„ ê°€ì´ë“œ(ëŒ€ì‹œë³´ë“œìš©)")
    st.write(
        """
- **ì‚°ì ë„**: xì¶• ECê°€ ì»¤ì§ˆìˆ˜ë¡ ìƒì¤‘ëŸ‰ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•˜ë©´ì„œ, **ì˜¨ë„(ë§ˆì»¤ í¬ê¸°)**ê¹Œì§€ í•¨ê»˜ ë¹„êµí•©ë‹ˆë‹¤.  
- **ìœµí•© êº¾ì€ì„ (ì´ì¤‘ì¶•)**: ë™ì¼í•œ xì¶•(EC) ìœ„ì—ì„œ **ìƒì¤‘ëŸ‰(ì¢Œì¶•)ê³¼ ì˜¨ë„(ìš°ì¶•)**ë¥¼ ë™ì‹œì— ë³´ë©´,
  â€˜ìƒì¤‘ëŸ‰ ë³€í™”ê°€ ì˜¨ë„ ë•Œë¬¸ì¸ì§€, EC ë•Œë¬¸ì¸ì§€â€™ë¥¼ ì§ê´€ì ìœ¼ë¡œ ë¶„ë¦¬í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
    )

    with st.expander("ğŸ“„ ë°ì´í„° í…Œì´ë¸” + CSV ë‹¤ìš´ë¡œë“œ"):
        st.dataframe(mline, use_container_width=True)
        csv_bytes = mline.to_csv(index=False).encode("utf-8-sig")
        st.download_button("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name="í•™êµë³„_ìš”ì•½ì§€í‘œ.csv", mime="text/csv")

st.caption("Â© Polar Plant Dashboard â€” Streamlit / Plotly")
